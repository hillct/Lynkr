# Lynkr Configuration
# Copy this file to .env and fill in your values

# ==============================================================================
# Model Provider Configuration
# ==============================================================================

# Primary model provider to use
# Options: databricks, azure-anthropic, azure-openai, openrouter, openai, ollama, llamacpp, lmstudio, bedrock, zai, vertex
# Default: databricks
MODEL_PROVIDER=ollama

# ==============================================================================
# Ollama Configuration (Hybrid Routing)
# ==============================================================================

# Enable Ollama preference for simple requests
PREFER_OLLAMA=false

# Ollama model to use (must be compatible with tool calling)
# Options: qwen2.5-coder:latest, llama3.1, mistral-nemo, nemotron-3-nano:30b-cloud, etc.
OLLAMA_MODEL=qwen2.5-coder:latest

# Ollama endpoint (default: http://localhost:11434)
OLLAMA_ENDPOINT=http://localhost:11434

# Ollama embeddings configuration (for Cursor @Codebase semantic search)
# Embedding models for local, privacy-first semantic search
# Popular models:
#   - nomic-embed-text (768 dim, 137M params, best all-around) - RECOMMENDED
#   - mxbai-embed-large (1024 dim, 335M params, higher quality)
#   - all-minilm (384 dim, 23M params, fastest/smallest)
#
# Pull model: ollama pull nomic-embed-text
# OLLAMA_EMBEDDINGS_MODEL=nomic-embed-text
# OLLAMA_EMBEDDINGS_ENDPOINT=http://localhost:11434/api/embeddings

# Fallback provider when primary provider fails or for complex requests
# Options: databricks, azure-anthropic, azure-openai, openrouter, openai, bedrock
# Note: Local providers (ollama, llamacpp, lmstudio) cannot be used as fallback
FALLBACK_PROVIDER=databricks

# Enable automatic fallback (true = transparent fallback, false = fail on provider error)
FALLBACK_ENABLED=false

# Max tools for routing to Ollama (requests with more tools go to cloud)
OLLAMA_MAX_TOOLS_FOR_ROUTING=3

# ==============================================================================
# Databricks Configuration
# ==============================================================================

# DATABRICKS_API_BASE=https://your-workspace.cloud.databricks.com
# DATABRICKS_API_KEY=dapi1234567890abcdef

# ==============================================================================
# AWS Bedrock Configuration
# ==============================================================================

# Bedrock API Key (Bearer token) - REQUIRED
# Generate from AWS Console → Bedrock → API Keys
# See: https://docs.aws.amazon.com/bedrock/latest/userguide/api-keys-generate.html
# AWS_BEDROCK_API_KEY=your-bedrock-api-key-here

# AWS region (default: us-east-1)
# Available regions: us-east-1, us-west-2, us-east-2, ap-southeast-1, ap-northeast-1, eu-central-1
# AWS_BEDROCK_REGION=us-east-2

# Bedrock model ID to use
# Claude models (recommended):
#   - anthropic.claude-3-5-sonnet-20241022-v2:0 (best for tool calling)
#   - anthropic.claude-3-opus-20240229-v1:0 (most capable)
#   - anthropic.claude-3-haiku-20240307-v1:0 (fast, cheap)
# Other models:
#   - us.deepseek.r1-v1:0 (DeepSeek R1 - reasoning model)
#   - qwen.qwen3-coder-480b-a35b-v1:0 (coding specialist)
#   - minimax.minimax-m2 (MiniMax M2)
#   - amazon.titan-text-express-v1
#   - meta.llama3-1-70b-instruct-v1:0
# AWS_BEDROCK_MODEL_ID=anthropic.claude-3-5-sonnet-20241022-v2:0

# ==============================================================================
# Azure Anthropic Configuration
# ==============================================================================

# AZURE_ANTHROPIC_ENDPOINT=https://your-anthropic.openai.azure.com
# AZURE_ANTHROPIC_API_KEY=your-azure-key

# ==============================================================================
# Azure OpenAI Configuration
# ==============================================================================

# Azure OpenAI endpoint (supports both standard and AI Foundry formats)
# Standard: https://<resource>.openai.azure.com
# AI Foundry: https://<resource>.services.ai.azure.com/models/chat/completions?api-version=2024-05-01-preview
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com

# AZURE_OPENAI_API_KEY=your-azure-openai-key
# AZURE_OPENAI_DEPLOYMENT=gpt-4o
# AZURE_OPENAI_API_VERSION=2024-05-01-preview

# ==============================================================================
# OpenAI Configuration (Direct OpenAI API)
# ==============================================================================

# OPENAI_API_KEY=sk-your-openai-api-key
# OPENAI_MODEL=gpt-4o
# OPENAI_ENDPOINT=https://api.openai.com/v1/chat/completions
# OPENAI_ORGANIZATION=org-your-org-id

# ==============================================================================
# OpenRouter Configuration (100+ Models via Single API)
# ==============================================================================

# Get your API key from: https://openrouter.ai/keys
# OPENROUTER_API_KEY=sk-or-v1-your-key-here

# Model to use (default: openai/gpt-4o-mini)
# Popular options:
#   - nvidia/nemotron-3-nano-30b-a3b:free (FREE)
#   - anthropic/claude-3.5-sonnet ($3/$15 per 1M)
#   - openai/gpt-4o-mini ($0.15/$0.60 per 1M)
# OPENROUTER_MODEL=openai/gpt-4o-mini

# ==============================================================================
# llama.cpp Configuration (Local GGUF Models)
# ==============================================================================

# LLAMACPP_ENDPOINT=http://localhost:8080
# LLAMACPP_MODEL=default
# LLAMACPP_TIMEOUT_MS=120000
# LLAMACPP_API_KEY=your-optional-api-key

# llama.cpp embeddings configuration
# LLAMACPP_EMBEDDINGS_ENDPOINT=http://localhost:8080/embeddings

# ==============================================================================
# LM Studio Configuration (Local Models with GUI)
# ==============================================================================

# LMSTUDIO_ENDPOINT=http://localhost:1234
# LMSTUDIO_MODEL=default
# LMSTUDIO_TIMEOUT_MS=120000
# LMSTUDIO_API_KEY=your-optional-api-key

# ==============================================================================
# Z.AI (Zhipu AI) Configuration - ~1/7 cost of Anthropic
# ==============================================================================

# Z.AI provides GLM models through an Anthropic-compatible API
# Get your API key from: https://z.ai/ or https://open.bigmodel.cn/
# ZAI_API_KEY=your-zai-api-key

# Z.AI endpoint (default: https://api.z.ai/api/anthropic/v1/messages)
# ZAI_ENDPOINT=https://api.z.ai/api/anthropic/v1/messages

# Model to use (GLM-4.7 is equivalent to Claude Sonnet, GLM-4.5-Air is like Haiku)
# Options: GLM-4.7, GLM-4.5-Air, GLM-4-Plus
# ZAI_MODEL=GLM-4.7

# ==============================================================================
# Google Vertex AI Configuration (Gemini Models)
# ==============================================================================

# Google AI API Key (required)
# Get your API key from: https://aistudio.google.com/app/apikey
# VERTEX_API_KEY=your-google-api-key
# or use: GOOGLE_API_KEY=your-google-api-key

# Gemini model to use (default: gemini-2.0-flash)
# Options:
#   - gemini-2.0-flash (fast, good for most tasks) - DEFAULT
#   - gemini-2.0-flash-lite (fastest, cheapest)
#   - gemini-2.5-pro (most capable, best quality)
#   - gemini-1.5-pro (previous generation)
#   - gemini-1.5-flash (previous generation, fast)
# VERTEX_MODEL=gemini-2.0-flash

# Model mapping from Claude names:
#   claude-sonnet-* → gemini-2.0-flash
#   claude-haiku-*  → gemini-2.0-flash-lite
#   claude-opus-*   → gemini-2.5-pro

# ==============================================================================
# Embeddings Provider Override
# ==============================================================================

# By default, embeddings use same provider as MODEL_PROVIDER (if supported)
# To force a specific provider, set:
# EMBEDDINGS_PROVIDER=ollama        # Use Ollama embeddings
# EMBEDDINGS_PROVIDER=llamacpp      # Use llama.cpp embeddings
# EMBEDDINGS_PROVIDER=openrouter    # Use OpenRouter embeddings
# EMBEDDINGS_PROVIDER=openai        # Use OpenAI embeddings

# ==============================================================================
# Server Configuration
# ==============================================================================

PORT=8081
LOG_LEVEL=info
WEB_SEARCH_ENDPOINT=http://localhost:8888/search

# Policy Configuration
POLICY_MAX_STEPS=20
POLICY_MAX_TOOL_CALLS=12

# Tool loop guard - max tool results in conversation before force-terminating
# Prevents infinite tool loops. Set higher for complex multi-step tasks.
POLICY_TOOL_LOOP_THRESHOLD=10

# Workspace for embeddings/indexing
WORKSPACE_ROOT=/path/to/your/workspace
WORKSPACE_INDEX_ENABLED=true

# Tool execution mode: where to execute tools (Write, Read, Bash, etc.)
# - server: Execute tools on the server (default, for standalone proxy use)
# - client/passthrough: Return tool calls to CLI for local execution
TOOL_EXECUTION_MODE=server

# Enable/disable automatic tool injection for local models
INJECT_TOOLS_LLAMACPP=true
INJECT_TOOLS_OLLAMA=true

# ==============================================================================
# Semantic Response Cache
# ==============================================================================

# Enable semantic caching (requires embeddings provider)
# Caches LLM responses and returns them for semantically similar prompts
# Requires: Ollama with nomic-embed-text, or another embeddings provider
SEMANTIC_CACHE_ENABLED=false

# Similarity threshold for cache hits (0.0-1.0, higher = stricter matching)
# 0.95 = very similar prompts only, 0.90 = more lenient
SEMANTIC_CACHE_THRESHOLD=0.95

# ==============================================================================
# Long-Term Memory System (Titans-Inspired)
# ==============================================================================

# Enable/disable the entire memory system
MEMORY_ENABLED=true

# Maximum number of memories to inject into each request
MEMORY_RETRIEVAL_LIMIT=5

# Minimum surprise score (0.0-1.0) required to store a memory
MEMORY_SURPRISE_THRESHOLD=0.3

# Auto-delete memories older than this many days
MEMORY_MAX_AGE_DAYS=90

# Maximum total memories to keep
MEMORY_MAX_COUNT=10000

# Enable importance decay over time
MEMORY_DECAY_ENABLED=true

# Days for importance to decay by 50%
MEMORY_DECAY_HALF_LIFE=30

# Include global memories in all sessions
MEMORY_INCLUDE_GLOBAL=true

# Where to inject memories (system or assistant_preamble)
MEMORY_INJECTION_FORMAT=system

# Enable automatic extraction
MEMORY_EXTRACTION_ENABLED=true

# ==============================================================================
# Token Optimization Settings (60-80% Cost Reduction)
# ==============================================================================

TOKEN_TRACKING_ENABLED=true
TOOL_TRUNCATION_ENABLED=true
MEMORY_FORMAT=compact
MEMORY_DEDUP_ENABLED=true
MEMORY_DEDUP_LOOKBACK=5
SYSTEM_PROMPT_MODE=dynamic
TOOL_DESCRIPTIONS=minimal
HISTORY_COMPRESSION_ENABLED=true
HISTORY_KEEP_RECENT_TURNS=10
HISTORY_SUMMARIZE_OLDER=true
TOKEN_BUDGET_WARNING=100000
TOKEN_BUDGET_MAX=180000
TOKEN_BUDGET_ENFORCEMENT=true

# ==============================================================================
# Smart Tool Selection (Advanced Token Optimization)
# ==============================================================================

# Selection strategy: heuristic, aggressive, or conservative
SMART_TOOL_SELECTION_MODE=heuristic

# Maximum token budget for tools per request
SMART_TOOL_SELECTION_TOKEN_BUDGET=2500

# ==============================================================================
# Performance & Security
# ==============================================================================

# API retry configuration
API_RETRY_MAX_RETRIES=3
API_RETRY_INITIAL_DELAY=1000
API_RETRY_MAX_DELAY=30000

# Load shedding thresholds
LOAD_SHEDDING_HEAP_THRESHOLD=90
LOAD_SHEDDING_EVENT_LOOP_DELAY=100

# ==============================================================================
# Hot Reload Configuration
# ==============================================================================

# Enable hot reload of configuration (default: true)
# When enabled, changes to .env are applied without restart
HOT_RELOAD_ENABLED=true

# Debounce delay in ms (prevents rapid reloads)
HOT_RELOAD_DEBOUNCE_MS=1000

# ==============================================================================
# Quick Start Examples
# ==============================================================================

# 100% Local (FREE) - Ollama:
#   MODEL_PROVIDER=ollama
#   OLLAMA_MODEL=qwen2.5-coder:latest
#   npm start

# AWS Bedrock:
#   MODEL_PROVIDER=bedrock
#   AWS_BEDROCK_API_KEY=your-key
#   AWS_BEDROCK_MODEL_ID=anthropic.claude-3-5-sonnet-20241022-v2:0
#   npm start

# OpenRouter (simplest cloud):
#   MODEL_PROVIDER=openrouter
#   OPENROUTER_API_KEY=sk-or-v1-your-key
#   npm start

# Azure OpenAI:
#   MODEL_PROVIDER=azure-openai
#   AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
#   AZURE_OPENAI_API_KEY=your-key
#   AZURE_OPENAI_DEPLOYMENT=gpt-4o
#   npm start

# Z.AI (Zhipu - ~1/7 cost of Anthropic):
#   MODEL_PROVIDER=zai
#   ZAI_API_KEY=your-zai-api-key
#   ZAI_MODEL=GLM-4.7
#   npm start

# Google Gemini (via Vertex AI):
#   MODEL_PROVIDER=vertex
#   VERTEX_API_KEY=your-google-api-key
#   VERTEX_MODEL=gemini-2.0-flash
#   npm start

# ==============================================================================
# Headroom Context Compression (Sidecar)
# ==============================================================================
# Headroom provides 47-92% token reduction through intelligent context compression.
# It runs as a Python sidecar container managed automatically by Lynkr via Docker.
#
# Features:
#   - Smart Crusher: Statistical JSON compression for tool outputs
#   - Cache Aligner: Stabilizes dynamic content for provider cache hits
#   - CCR (Compress-Cache-Retrieve): Reversible compression with on-demand retrieval
#   - Rolling Window: Token budget enforcement with turn-based windowing
#   - LLMLingua (optional): ML-based 20x compression with GPU acceleration

# Enable/disable Headroom compression (default: false)
HEADROOM_ENABLED=false

# Sidecar endpoint (auto-configured when Docker is enabled)
HEADROOM_ENDPOINT=http://localhost:8787

# Request timeout in milliseconds
HEADROOM_TIMEOUT_MS=5000

# Minimum tokens to trigger compression (skip small requests)
HEADROOM_MIN_TOKENS=500

# Operating mode: "audit" (observe only) or "optimize" (apply transforms)
HEADROOM_MODE=optimize

# Provider for cache optimization hints: anthropic, openai, google
HEADROOM_PROVIDER=anthropic

# Log level: debug, info, warning, error
HEADROOM_LOG_LEVEL=info

# ==============================================================================
# Headroom Docker Configuration
# ==============================================================================
# When enabled, Lynkr automatically manages the Headroom container lifecycle

# Enable Docker container management (default: true when HEADROOM_ENABLED=true)
HEADROOM_DOCKER_ENABLED=true

# Docker image to use
HEADROOM_DOCKER_IMAGE=lynkr/headroom-sidecar:latest

# Container name
HEADROOM_DOCKER_CONTAINER_NAME=lynkr-headroom

# Port mapping
HEADROOM_DOCKER_PORT=8787

# Resource limits
HEADROOM_DOCKER_MEMORY_LIMIT=512m
HEADROOM_DOCKER_CPU_LIMIT=1.0

# Restart policy: no, always, unless-stopped, on-failure
HEADROOM_DOCKER_RESTART_POLICY=unless-stopped

# Docker network (optional, leave empty for default bridge)
# HEADROOM_DOCKER_NETWORK=lynkr-network

# Build from local source instead of pulling image
# HEADROOM_DOCKER_AUTO_BUILD=true
# HEADROOM_DOCKER_BUILD_CONTEXT=./headroom-sidecar

# ==============================================================================
# Headroom Transform Settings
# ==============================================================================

# Smart Crusher (statistical JSON compression)
HEADROOM_SMART_CRUSHER=true
HEADROOM_SMART_CRUSHER_MIN_TOKENS=200
HEADROOM_SMART_CRUSHER_MAX_ITEMS=15

# Tool Crusher (fixed-rules compression for tool outputs)
HEADROOM_TOOL_CRUSHER=true

# Cache Aligner (stabilize dynamic content like UUIDs, timestamps)
HEADROOM_CACHE_ALIGNER=true

# Rolling Window (context overflow management)
HEADROOM_ROLLING_WINDOW=true
HEADROOM_KEEP_TURNS=3

# ==============================================================================
# Headroom CCR (Compress-Cache-Retrieve)
# ==============================================================================

# Enable CCR for reversible compression with on-demand retrieval
HEADROOM_CCR=true

# TTL for cached content in seconds (default: 5 minutes)
HEADROOM_CCR_TTL=300

# ==============================================================================
# Headroom LLMLingua (Optional ML Compression)
# ==============================================================================
# LLMLingua-2 provides ML-based 20x compression using BERT token classification.
# Requires GPU for reasonable performance, or use CPU with longer timeouts.

# Enable LLMLingua (default: false, requires GPU recommended)
HEADROOM_LLMLINGUA=false

# Device: cuda, cpu, auto
HEADROOM_LLMLINGUA_DEVICE=auto

# ==============================================================================
# Prompt Cache Configuration
# ==============================================================================

# Enable prompt caching (caches exact prompts)
PROMPT_CACHE_ENABLED=true
PROMPT_CACHE_MAX_ENTRIES=1000
PROMPT_CACHE_TTL_MS=300000
