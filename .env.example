# Lynkr + Ollama Configuration Example
# Copy this file to .env and fill in your values

# ==============================================================================
# Model Provider Configuration
# ==============================================================================

# Primary model provider to use
# Options: databricks, azure-anthropic, azure-openai, openrouter, openai, ollama, llamacpp, lmstudio, bedrock
# Default: databricks
# MODEL_PROVIDER=databricks

# ==============================================================================
# Ollama Configuration (Hybrid Routing)
# ==============================================================================

# Enable Ollama preference for simple requests
PREFER_OLLAMA=true

# Ollama model to use (must be compatible with tool calling)
# Options: qwen2.5-coder:latest, llama3.1, mistral-nemo, etc.
OLLAMA_MODEL=qwen2.5-coder:latest

# Ollama endpoint (default: http://localhost:11434)
# OLLAMA_ENDPOINT=http://localhost:11434

# Request timeout in milliseconds (default: 120000)
# OLLAMA_TIMEOUT_MS=120000

# Ollama embeddings configuration (for Cursor @Codebase semantic search)
# Embedding models for local, privacy-first semantic search
# Popular models:
#   - nomic-embed-text (768 dim, 137M params, best all-around) - RECOMMENDED
#   - mxbai-embed-large (1024 dim, 335M params, higher quality)
#   - all-minilm (384 dim, 23M params, fastest/smallest)
#
# Pull model: ollama pull nomic-embed-text
# OLLAMA_EMBEDDINGS_MODEL=nomic-embed-text
# OLLAMA_EMBEDDINGS_ENDPOINT=http://localhost:11434/api/embeddings

# Fallback provider when primary provider fails or for complex requests
# Options: databricks, azure-anthropic, azure-openai, openrouter, openai, bedrock
# Note: Local providers (ollama, llamacpp, lmstudio) cannot be used as fallback
FALLBACK_PROVIDER=databricks

# Enable automatic fallback (true = transparent fallback, false = fail on provider error)
FALLBACK_ENABLED=true

# Max tools for routing to Ollama (requests with more tools go to cloud)
OLLAMA_MAX_TOOLS_FOR_ROUTING=3

# ==============================================================================
# Databricks Configuration (Required for Fallback)
# ==============================================================================

# DATABRICKS_API_BASE=https://your-workspace.cloud.databricks.com
# DATABRICKS_API_KEY=dapi1234567890abcdef

# ==============================================================================
# Azure Anthropic Configuration (Optional Alternative Fallback)
# ==============================================================================

# AZURE_ANTHROPIC_ENDPOINT=https://your-anthropic.openai.azure.com
# AZURE_ANTHROPIC_API_KEY=your-azure-key

# ==============================================================================
# Azure OpenAI Configuration (Optional Alternative Fallback)
# ==============================================================================

# Azure OpenAI resource endpoint (from Azure Portal)
# Format: https://<your-resource-name>.openai.azure.com
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com

# Azure OpenAI API key (found in Azure Portal under Keys and Endpoint)
# AZURE_OPENAI_API_KEY=your-azure-openai-key

# Deployment name (the model deployment you created in Azure)
# This can point to any Azure OpenAI model: gpt-4o, gpt-5, gpt-5-codex, etc.
# Default: gpt-4o
# AZURE_OPENAI_DEPLOYMENT=gpt-4o

# API version (use latest stable version for best features)
# Default: 2024-08-01-preview
# AZURE_OPENAI_API_VERSION=2024-08-01-preview

# ==============================================================================
# OpenAI Configuration (Direct OpenAI API)
# ==============================================================================

# OpenAI API key (from https://platform.openai.com/api-keys)
# OPENAI_API_KEY=sk-your-openai-api-key

# OpenAI model to use
# Options: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo, o1-preview, o1-mini
# Default: gpt-4o
# OPENAI_MODEL=gpt-4o

# OpenAI API endpoint (usually don't need to change this)
# Default: https://api.openai.com/v1/chat/completions
# OPENAI_ENDPOINT=https://api.openai.com/v1/chat/completions

# OpenAI organization ID (optional, for organization-level API keys)
# OPENAI_ORGANIZATION=org-your-org-id

# ==============================================================================
# OpenRouter Configuration (100+ Models via Single API)
# ==============================================================================

# OpenRouter is a unified API gateway that provides access to 100+ AI models
# from multiple providers (Anthropic, OpenAI, Google, Meta, etc.) with a single API key.
#
# Benefits:
#   ‚úÖ ONE key for chat + embeddings (simplest Cursor setup)
#   ‚úÖ 100+ models: Claude, GPT, Gemini, Llama, Mistral, etc.
#   ‚úÖ Competitive pricing: Often cheaper than direct providers
#   ‚úÖ No rate limits: Aggregates across providers
#   ‚úÖ Automatic fallbacks: Switches providers if one is down
#
# Get your API key from: https://openrouter.ai/keys
# View all models: https://openrouter.ai/models

# OpenRouter API key (REQUIRED)
# OPENROUTER_API_KEY=sk-or-v1-your-key-here

# Model to use (default: openai/gpt-4o-mini)
#
# ===== CLAUDE MODELS (BEST FOR CODING) =====
#   anthropic/claude-3.5-sonnet       - $3/$15 per 1M tokens (excellent for tool calling)
#   anthropic/claude-opus-4.5         - $15/$75 per 1M tokens (most capable)
#   anthropic/claude-3-haiku          - $0.25/$1.25 per 1M tokens (fast, cheap)
#
# ===== OPENAI MODELS =====
#   openai/gpt-4o                     - $2.50/$10 per 1M tokens (multimodal)
#   openai/gpt-4o-mini                - $0.15/$0.60 per 1M tokens (cheapest, default)
#   openai/o1-preview                 - $15/$60 per 1M tokens (reasoning model)
#   openai/o1-mini                    - $3/$12 per 1M tokens (reasoning, cheaper)
#
# ===== GOOGLE MODELS =====
#   google/gemini-pro-1.5             - $1.25/$5 per 1M tokens
#   google/gemini-flash-1.5           - $0.075/$0.30 per 1M tokens (fastest)
#
# ===== META MODELS =====
#   meta-llama/llama-3.1-405b         - $2.70/$2.70 per 1M tokens (largest)
#   meta-llama/llama-3.1-70b          - $0.52/$0.75 per 1M tokens
#   meta-llama/llama-3.1-8b           - $0.06/$0.06 per 1M tokens (cheapest)
#
# ===== MISTRAL MODELS =====
#   mistralai/mistral-large            - $2/$6 per 1M tokens
#   mistralai/codestral-latest         - $0.30/$0.90 per 1M tokens (coding specialist)
#
# ===== DEEPSEEK MODELS =====
#   deepseek/deepseek-chat             - $0.14/$0.28 per 1M tokens
#   deepseek/deepseek-coder            - $0.14/$0.28 per 1M tokens (coding)
#
# View pricing & all models: https://openrouter.ai/models
# OPENROUTER_MODEL=openai/gpt-4o-mini

# Embeddings model to use for Cursor @Codebase search (default: openai/text-embedding-ada-002)
# Separate from chat model for cost/quality optimization
#
# ===== EMBEDDINGS MODELS =====
#   openai/text-embedding-ada-002      - $0.10 per 1M tokens (standard, widely supported)
#   openai/text-embedding-3-small      - $0.02 per 1M tokens (80% cheaper! recommended)
#   openai/text-embedding-3-large      - $0.13 per 1M tokens (better quality, 3072 dimensions)
#   voyage/voyage-2                     - $0.12 per 1M tokens (best quality for code search)
#   voyage/voyage-code-2                - $0.12 per 1M tokens (specialized for code)
#
# Cost optimization example:
#   OPENROUTER_MODEL=anthropic/claude-3.5-sonnet           # $3/$15 per 1M (chat)
#   OPENROUTER_EMBEDDINGS_MODEL=openai/text-embedding-3-small  # $0.02 per 1M (embeddings)
#   Result: Best chat quality + 80% cheaper embeddings!
#
# OPENROUTER_EMBEDDINGS_MODEL=openai/text-embedding-ada-002

# OpenRouter API endpoint (usually don't need to change this)
# Default: https://openrouter.ai/api/v1/chat/completions
# OPENROUTER_ENDPOINT=https://openrouter.ai/api/v1/chat/completions

# Maximum tools for routing to OpenRouter in hybrid mode
# Requests with more tools than this go to other providers
# Default: 15 (OpenRouter handles moderate tool counts well)
# OPENROUTER_MAX_TOOLS_FOR_ROUTING=15

# ==============================================================================
# llama.cpp Configuration (Local GGUF Models)
# ==============================================================================

# llama.cpp server endpoint (default: http://localhost:8080)
# Start with: ./llama-server -m model.gguf --port 8080
# LLAMACPP_ENDPOINT=http://localhost:8080

# Model name (for logging purposes, llama.cpp uses the loaded model)
# LLAMACPP_MODEL=default

# Request timeout in milliseconds (default: 120000)
# LLAMACPP_TIMEOUT_MS=120000

# Optional API key (for secured llama.cpp servers)
# LLAMACPP_API_KEY=your-optional-api-key

# llama.cpp embeddings configuration (for Cursor @Codebase semantic search)
# Requires an embedding model loaded in llama.cpp server
# Example GGUF embedding models:
#   - nomic-embed-text-v1.5.Q4_K_M.gguf (recommended, 768 dim)
#   - all-MiniLM-L6-v2.Q4_K_M.gguf (smallest, fastest, 384 dim)
#   - bge-large-en-v1.5.Q4_K_M.gguf (highest quality, 1024 dim)
#
# Start with: ./llama-server -m nomic-embed-text-v1.5.Q4_K_M.gguf --port 8080 --embedding
# LLAMACPP_EMBEDDINGS_ENDPOINT=http://localhost:8080/embeddings

# ==============================================================================
# LM Studio Configuration (Local Models with GUI)
# ==============================================================================

# LM Studio server endpoint (default: http://localhost:1234)
# Start LM Studio and load a model, it will start the server automatically
# LMSTUDIO_ENDPOINT=http://localhost:1234

# Model name (for logging purposes, LM Studio uses the loaded model)
# LMSTUDIO_MODEL=default

# Request timeout in milliseconds (default: 120000)
# LMSTUDIO_TIMEOUT_MS=120000

# Optional API key (for secured LM Studio servers)
# LMSTUDIO_API_KEY=your-optional-api-key

# ==============================================================================
# AWS Bedrock Configuration (Claude, Titan, Llama, Jurassic, etc.)
# ==============================================================================

# Bedrock API Key (Bearer token) - REQUIRED
# Generate from AWS Console ‚Üí Bedrock ‚Üí API Keys
# See: https://docs.aws.amazon.com/bedrock/latest/userguide/api-keys-generate.html
# AWS_BEDROCK_API_KEY=your-bedrock-api-key-here

# AWS region (default: us-east-1)
# Available regions: us-east-1, us-west-2, us-east-2, ap-southeast-1, ap-northeast-1, eu-central-1
# AWS_BEDROCK_REGION=us-east-1

# Bedrock model ID to use (via Converse API - unified format for all text models)
#
# ===== CLAUDE MODELS (BEST FOR TOOL CALLING) =====
# Claude 4.5 models (latest - requires inference profiles):
#   - us.anthropic.claude-sonnet-4-5-20250929-v1:0 (regional US, best for tool calling)
#   - us.anthropic.claude-haiku-4-5-20251001-v1:0 (fast, efficient)
#   - global.anthropic.claude-sonnet-4-5-20250929-v1:0 (cross-region)
# Claude 3.x models:
#   - anthropic.claude-3-5-sonnet-20241022-v2:0 (excellent for tool calling)
#   - anthropic.claude-3-opus-20240229-v1:0 (most capable)
#   - anthropic.claude-3-haiku-20240307-v1:0 (fast, cheap)
#
# ===== DEEPSEEK MODELS (NEW - 2025) =====
#   - us.deepseek.r1-v1:0 (DeepSeek R1 - reasoning model)
#   - deepseek.r1-v1:0 (direct model ID)
#
# ===== QWEN MODELS (ALIBABA - NEW 2025) =====
#   - qwen.qwen3-235b-a22b-2507-v1:0 (largest, 235B parameters)
#   - qwen.qwen3-32b-v1:0 (balanced, 32B parameters)
#   - qwen.qwen3-coder-480b-a35b-v1:0 (coding specialist, 480B)
#   - qwen.qwen3-coder-30b-a3b-v1:0 (coding, smaller)
#
# ===== OPENAI OPEN-WEIGHT MODELS (NEW - 2025) =====
#   - openai.gpt-oss-120b-1:0 (120B parameters, open-weight)
#   - openai.gpt-oss-20b-1:0 (20B parameters, efficient)
#
# ===== GOOGLE GEMMA MODELS (OPEN-WEIGHT) =====
#   - google.gemma-3-27b (27B parameters)
#   - google.gemma-3-12b (12B parameters)
#   - google.gemma-3-4b (4B parameters, efficient)
#
# ===== MINIMAX MODELS (NEW - 2025) =====
#   - minimax.m2-v1:0 (MiniMax M2)
#
# ===== AMAZON MODELS =====
# Nova (multimodal):
#   - us.amazon.nova-pro-v1:0 (best quality, multimodal)
#   - us.amazon.nova-lite-v1:0 (fast, cost-effective)
#   - us.amazon.nova-micro-v1:0 (ultra-fast, text-only)
# Titan:
#   - amazon.titan-text-premier-v1:0 (largest)
#   - amazon.titan-text-express-v1 (fast)
#   - amazon.titan-text-lite-v1 (cheapest)
#
# ===== META LLAMA MODELS =====
#   - meta.llama3-1-70b-instruct-v1:0 (most capable)
#   - meta.llama3-1-8b-instruct-v1:0 (fast, efficient)
#
# ===== MISTRAL MODELS =====
#   - mistral.mistral-large-2407-v1:0 (largest)
#   - mistral.mistral-small-2402-v1:0 (efficient)
#   - mistral.mixtral-8x7b-instruct-v0:1 (mixture of experts)
#
# ===== COHERE COMMAND MODELS =====
#   - cohere.command-r-plus-v1:0 (best for RAG)
#   - cohere.command-r-v1:0 (balanced)
#
# ===== AI21 MODELS =====
#   - ai21.jamba-1-5-large-v1:0 (hybrid architecture)
#   - ai21.jamba-1-5-mini-v1:0 (fast)
#
# ===== OTHER PROVIDERS (CHECK AWS CONSOLE FOR AVAILABILITY) =====
# Writer AI, Kimi AI, Luma AI, TwelveLabs models may also be available
#
# Default (Claude Sonnet 4.5 with tool calling):
# AWS_BEDROCK_MODEL_ID=us.anthropic.claude-sonnet-4-5-20250929-v1:0

# Cost comparison (per 1M tokens):
# Claude 3.5 Sonnet on Bedrock: $3.00 input / $15.00 output
# Claude 3 Opus on Bedrock: $15.00 input / $75.00 output
# Titan Text Express: $0.20 input / $0.60 output
# Llama 3 70B: $0.99 input / $0.99 output

# ==============================================================================
# Server Configuration
# ==============================================================================

PORT=8080
LOG_LEVEL=info
WEB_SEARCH_ENDPOINT=http://localhost:8888/search

# Tool execution mode: where to execute tools (Write, Read, Bash, etc.)
# - server: Execute tools on the server (default, for standalone proxy use)
# - passthrough: Return tool calls to CLI for local execution (for Claude Code CLI compatibility)
# TOOL_EXECUTION_MODE=server

# ==============================================================================
# Subagent Configuration (Server Mode - matches Claude Code architecture)
# ==============================================================================

# Enable server-side subagents
# When enabled, the main agent can spawn specialized subagents for complex tasks
# AGENTS_ENABLED=true

# Max concurrent subagents (Claude Code uses up to 10, batched)
# AGENTS_MAX_CONCURRENT=10

# Default model for subagents
# Options: haiku (fast/cheap), sonnet (smart), gpt-4o-mini, gpt-4o
# AGENTS_DEFAULT_MODEL=haiku

# Max steps per subagent
# AGENTS_MAX_STEPS=15

# Timeout per subagent (milliseconds)
# AGENTS_TIMEOUT=120000

# ==============================================================================
# Long-Term Memory System (Titans-Inspired)
# ==============================================================================

# Enable/disable the entire memory system
# When enabled, automatically extracts and retrieves conversation memories
# Default: true
# MEMORY_ENABLED=true

# Maximum number of memories to inject into each request
# Higher = more context but larger prompts
# Default: 5, Range: 1-20
# MEMORY_RETRIEVAL_LIMIT=5

# Minimum surprise score (0.0-1.0) required to store a memory
# Filters out redundant information
# Lower (0.1-0.2) = store more memories
# Higher (0.4-0.5) = only store highly novel information
# Default: 0.3
# MEMORY_SURPRISE_THRESHOLD=0.3

# Auto-delete memories older than this many days
# Prevents database bloat with stale information
# Default: 90
# MEMORY_MAX_AGE_DAYS=90

# Maximum total memories to keep (prunes least important when exceeded)
# Default: 10000
# MEMORY_MAX_COUNT=10000

# Enable importance decay over time (exponential decay)
# Memories become less important as they age
# Default: true
# MEMORY_DECAY_ENABLED=true

# Days for importance to decay by 50% (exponential half-life)
# Shorter (7-14) = prefer recent info
# Longer (60-90) = value historical context
# Default: 30
# MEMORY_DECAY_HALF_LIFE=30

# Include memories with session_id=NULL (global memories) in all sessions
# Global memories = project facts shared across all conversations
# Default: true
# MEMORY_INCLUDE_GLOBAL=true

# Where to inject memories in the prompt
# Options: system (recommended), assistant_preamble
# Default: system
# MEMORY_INJECTION_FORMAT=system

# Enable automatic extraction of memories from assistant responses
# Disable for read-only memory mode
# Default: true
# MEMORY_EXTRACTION_ENABLED=true

# ==============================================================================
# Token Optimization Settings (60-80% Cost Reduction)
# ==============================================================================

# Enable tracking of estimated and actual token usage per request
# Provides visibility into token consumption patterns
# Default: true
# TOKEN_TRACKING_ENABLED=true

# Automatically truncate tool outputs to reduce token usage
# Read: 8k chars, Bash: 30k chars, Grep: 12k chars
# Default: true
# TOOL_TRUNCATION_ENABLED=true

# Memory format: 'compact' (50 tokens) or 'verbose' (250 tokens)
# Compact format saves ~75% of memory-related tokens
# Default: compact
# MEMORY_FORMAT=compact

# Enable deduplication of memories with recent conversation
# Filters out memories already present in last N messages
# Default: true
# MEMORY_DEDUP_ENABLED=true

# Number of recent messages to check for memory deduplication
# Default: 5
# MEMORY_DEDUP_LOOKBACK=5

# System prompt mode: 'dynamic' (adaptive) or 'full' (static)
# Dynamic mode includes only relevant sections based on context
# Saves 500-1000 tokens per request
# Default: dynamic
# SYSTEM_PROMPT_MODE=dynamic

# Tool descriptions: 'minimal' (concise) or 'full' (verbose)
# Minimal descriptions save 200-300 tokens per request
# Default: minimal
# TOOL_DESCRIPTIONS=minimal

# Enable automatic compression of old conversation history
# Summarizes older turns while keeping recent context verbatim
# Default: true
# HISTORY_COMPRESSION_ENABLED=true

# Number of recent turns to keep verbatim (not compressed)
# Default: 10
# HISTORY_KEEP_RECENT_TURNS=10

# Summarize older conversation turns beyond recent window
# Default: true
# HISTORY_SUMMARIZE_OLDER=true

# Warn when approaching token budget (logged, not blocking)
# Default: 100000 tokens
# TOKEN_BUDGET_WARNING=100000

# Maximum token budget per request
# Default: 180000 tokens (leaves buffer for 20k output)
# TOKEN_BUDGET_MAX=180000

# Enforce token budget with adaptive compression
# Automatically applies additional compression when approaching limit
# Default: true
# TOKEN_BUDGET_ENFORCEMENT=true

# ==============================================================================
# Smart Tool Selection (Advanced Token Optimization)
# ==============================================================================

# Smart tool selection is ALWAYS ENABLED and reduces tool count from 12 ‚Üí 0-6
# tools depending on query complexity
# Potential savings: 500-2000 tokens per request (10-30% reduction)

# Selection strategy
# - heuristic: Balanced approach (recommended, default)
# - aggressive: Maximum token savings, higher risk
# - conservative: Safer, includes more tools when uncertain
# SMART_TOOL_SELECTION_MODE=heuristic

# Maximum token budget for tools per request
# Default: 2500 tokens (~10 tools)
# Lower values force more aggressive filtering
# SMART_TOOL_SELECTION_TOKEN_BUDGET=2500

# ==============================================================================
# Cursor IDE Integration (OpenAI API Compatibility)
# ==============================================================================

# Lynkr provides full Cursor IDE support via OpenAI-compatible endpoints:
#   - POST /v1/chat/completions - Chat with streaming
#   - GET /v1/models - Model listing
#   - POST /v1/embeddings - Semantic search (requires OpenRouter or OpenAI API key)
#   - GET /v1/health - Health check

# ===== CURSOR SETUP =====
#
# 1. Start Lynkr with your preferred provider (Databricks, Bedrock, OpenRouter, etc.)
#    lynkr start
#
# 2. In Cursor, go to Settings ‚Üí Models ‚Üí OpenAI API
#    - API Key: any-non-empty-value (Cursor requires this, but Lynkr ignores it)
#    - Base URL: http://localhost:8080/v1
#    - Model: claude-sonnet-4.5 (or your configured model)
#
# 3. (Optional) Enable embeddings for semantic search:
#    - If using OpenRouter: Already works! (same key handles both)
#    - If using other providers: Set OPENROUTER_API_KEY or OPENAI_API_KEY below
#
# 4. Test in Cursor:
#    - Chat works immediately
#    - @Codebase search works if you have embeddings (step 3)

# ===== EMBEDDINGS FOR @CODEBASE SEARCH =====
#
# Cursor's @Codebase semantic search requires embeddings.
#
# ‚ö° ALREADY USING OPENROUTER? You're all set!
#    If MODEL_PROVIDER=openrouter, embeddings work automatically with the same key.
#    No additional configuration needed.
#
# üîß Using a different provider (Databricks, Bedrock, Ollama, etc.)?
#    Choose ONE of these for embeddings (ordered by privacy):
#
# Option A: Ollama (100% local, FREE, most private) - RECOMMENDED for privacy
#   ollama pull nomic-embed-text
#   OLLAMA_EMBEDDINGS_MODEL=nomic-embed-text
#   Cost: FREE, Privacy: 100% local, Quality: Good
#
# Option B: llama.cpp (100% local, FREE, GGUF models)
#   ./llama-server -m nomic-embed-text-v1.5.Q4_K_M.gguf --port 8080 --embedding
#   LLAMACPP_EMBEDDINGS_ENDPOINT=http://localhost:8080/embeddings
#   Cost: FREE, Privacy: 100% local, Quality: Good
#
# Option C: OpenRouter (cloud, cheapest cloud option)
#   OPENROUTER_API_KEY=sk-or-v1-...
#   Get key from: https://openrouter.ai/keys
#   Cost: ~$0.0001 per 1K tokens (~$0.01-0.10/month), Privacy: Cloud, Quality: Excellent
#
# Option D: OpenAI directly (cloud)
#   OPENAI_API_KEY=sk-...
#   Get key from: https://platform.openai.com/api-keys
#   Cost: ~$0.0001 per 1K tokens, Privacy: Cloud, Quality: Excellent
#
# Without embeddings:
#   ‚úÖ Chat, completions, Cmd+K work
#   ‚ùå @Codebase semantic search doesn't work

# ===== FEATURES MATRIX =====
#
# With Lynkr + Cursor (NO embeddings):
#   ‚úÖ Chat in current file
#   ‚úÖ Inline autocomplete
#   ‚úÖ Cmd+K edits
#   ‚úÖ Manual @file references
#   ‚úÖ Terminal commands
#   ‚ùå @Codebase semantic search
#   ‚ùå Automatic context inclusion
#
# With Lynkr + Cursor + Embeddings (e.g., OpenRouter):
#   ‚úÖ All above features
#   ‚úÖ @Codebase semantic search
#   ‚úÖ Automatic context inclusion
#   ‚úÖ "Find similar code"
#
# Cost comparison (100K requests/month):
#   Cursor native: ~$20-50/month (GPT-4)
#   Lynkr + Databricks: ~$15-30/month (chat + separate embeddings)
#   Lynkr + OpenRouter: ~$5-10/month (ONE key for chat + embeddings ‚ö°)
#   Lynkr + Ollama: FREE (local chat, add OpenRouter key for embeddings: ~$0.10/month)

# ==============================================================================
# Performance & Security
# ==============================================================================

# Rate limiting configuration
# Enable/disable rate limiting (default: true)
# RATE_LIMIT_ENABLED=true

# Rate limit window in milliseconds (default: 60000 = 1 minute)
# RATE_LIMIT_WINDOW_MS=60000

# Maximum requests per window (default: 100)
# RATE_LIMIT_MAX=100

# Rate limit key strategy (default: session)
# Options: session (per session ID), ip (per IP address), both (per session+IP)
# RATE_LIMIT_KEY_BY=session

# API retry configuration
# API_RETRY_MAX_RETRIES=3
# API_RETRY_INITIAL_DELAY=1000
# API_RETRY_MAX_DELAY=30000

# Load shedding thresholds (percentage, 0-100)
# LOAD_SHEDDING_HEAP_THRESHOLD=90
# LOAD_SHEDDING_EVENT_LOOP_DELAY=100

# ==============================================================================
# Notes
# ==============================================================================

# For Docker Compose:
# 1. Copy this file to .env
# 2. Fill in your Databricks credentials
# 3. Run: docker-compose up -d
# 4. Wait for Ollama to download the model (first run takes time)
# 5. Access Lynkr at http://localhost:8080

# For standalone Ollama (no Docker):
# 1. Install Ollama: https://ollama.ai/download
# 2. Pull model: ollama pull qwen2.5-coder:latest
# 3. Set OLLAMA_ENDPOINT to http://localhost:11434
# 4. Run Lynkr: npm start
