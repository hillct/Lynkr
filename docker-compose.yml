services:
  # Lynkr proxy service
  lynkr:
    build: .
    container_name: lynkr
    image: lynkr:2.0.0
    ports:
      - "8081:8080"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      # ============================================================
      # PRIMARY MODEL PROVIDER
      # ============================================================
      # Options: ollama, databricks, azure-openai, azure-anthropic, openrouter, bedrock, llamacpp, lmstudio, openai
      # - ollama: Local models (free, private, offline)
      # - databricks: Claude Sonnet 4.5, Opus 4.5 (production)
      # - azure-openai: GPT-4o, GPT-5, o1, o3 (Azure integration)
      # - azure-anthropic: Claude models via Azure
      # - openrouter: 100+ models (flexible, cost-effective)
      # - bedrock: AWS Bedrock (Claude, Titan, Llama, etc.)
      # - llamacpp: Local GGUF models
      # - lmstudio: LM Studio local models
      # - openai: Direct OpenAI API
      MODEL_PROVIDER: ${MODEL_PROVIDER:-ollama}

      # ============================================================
      # TOOL EXECUTION MODE
      # ============================================================
      # Options: server (default), client (passthrough mode)
      # - server: Tools execute on proxy server
      # - client: Tools execute on Claude Code CLI (client-side)
      TOOL_EXECUTION_MODE: ${TOOL_EXECUTION_MODE:-server}

      # ============================================================
      # OLLAMA CONFIGURATION (Local Models)
      # ============================================================
      # Recommended models for tool calling:
      # - llama3.1:8b (good balance)
      # - llama3.2 (latest)
      # - qwen2.5:14b (strong reasoning, 7b struggles with tools)
      # - mistral:7b-instruct (fast and capable)
      PREFER_OLLAMA: ${PREFER_OLLAMA:-true}
#      OLLAMA_ENDPOINT: http://ollama:11434
      OLLAMA_ENDPOINT: http://host.docker.internal:11434
      OLLAMA_MODEL: ${OLLAMA_MODEL:-llama3.1:8b}
      OLLAMA_MAX_TOOLS_FOR_ROUTING: ${OLLAMA_MAX_TOOLS_FOR_ROUTING:-3}
      # Ollama Embeddings (for Cursor @Codebase search)
      OLLAMA_EMBEDDINGS_MODEL: ${OLLAMA_EMBEDDINGS_MODEL:-nomic-embed-text}
#      OLLAMA_EMBEDDINGS_ENDPOINT: ${OLLAMA_EMBEDDINGS_ENDPOINT:-http://ollama:11434/api/embeddings}
      OLLAMA_EMBEDDINGS_ENDPOINT: ${OLLAMA_EMBEDDINGS_ENDPOINT:-http://host.docker.internal:11434/api/embeddings}

      # ============================================================
      # OPENROUTER CONFIGURATION
      # ============================================================
      # Get API key from: https://openrouter.ai/keys
      # Popular models: openai/gpt-4o-mini, anthropic/claude-3.5-sonnet
      OPENROUTER_API_KEY: ${OPENROUTER_API_KEY:-}
      OPENROUTER_MODEL: ${OPENROUTER_MODEL:-amazon/nova-2-lite-v1:free}
      OPENROUTER_EMBEDDINGS_MODEL: ${OPENROUTER_EMBEDDINGS_MODEL:-openai/text-embedding-ada-002}
      OPENROUTER_ENDPOINT: ${OPENROUTER_ENDPOINT:-https://openrouter.ai/api/v1/chat/completions}
      OPENROUTER_MAX_TOOLS_FOR_ROUTING: ${OPENROUTER_MAX_TOOLS_FOR_ROUTING:-15}

      # ============================================================
      # AZURE OPENAI CONFIGURATION
      # ============================================================
      # Required when MODEL_PROVIDER=azure-openai
      # IMPORTANT: Use FULL endpoint URL including deployment path and API version
      # Format: https://YOUR-RESOURCE.openai.azure.com/openai/deployments/YOUR-DEPLOYMENT/chat/completions?api-version=2025-01-01-preview
      # Get credentials from: https://portal.azure.com → Azure OpenAI → Keys and Endpoint
      # Deployment options: gpt-4o, gpt-4o-mini, gpt-5-chat, o1-preview, o3-mini
      AZURE_OPENAI_ENDPOINT: ${AZURE_OPENAI_ENDPOINT:-}
      AZURE_OPENAI_API_KEY: ${AZURE_OPENAI_API_KEY:-}
      AZURE_OPENAI_DEPLOYMENT: ${AZURE_OPENAI_DEPLOYMENT:-gpt-4o}

      # ============================================================
      # HYBRID ROUTING & FALLBACK
      # ============================================================
      # Enable/disable fallback to cloud providers
      FALLBACK_ENABLED: ${FALLBACK_ENABLED:-true}
      # Fallback provider when Ollama can't handle request
      # Options: databricks, azure-openai, azure-anthropic, openrouter, bedrock, openai
      # Note: Local providers (ollama, llamacpp, lmstudio) cannot be used as fallback
      FALLBACK_PROVIDER: ${FALLBACK_PROVIDER:-databricks}

      # ============================================================
      # DATABRICKS CONFIGURATION
      # ============================================================
      DATABRICKS_API_BASE: ${DATABRICKS_API_BASE:-https://example.cloud.databricks.com}
      DATABRICKS_API_KEY: ${DATABRICKS_API_KEY:-replace-with-databricks-pat}

      # ============================================================
      # AZURE ANTHROPIC CONFIGURATION (OPTIONAL)
      # ============================================================
      AZURE_ANTHROPIC_ENDPOINT: ${AZURE_ANTHROPIC_ENDPOINT:-}
      AZURE_ANTHROPIC_API_KEY: ${AZURE_ANTHROPIC_API_KEY:-}

      # ============================================================
      # AWS BEDROCK CONFIGURATION (OPTIONAL)
      # ============================================================
      # Supports Claude, Titan, Llama, Jurassic, Cohere, Mistral models
      # Get API key from AWS Console → Bedrock → API Keys
      AWS_BEDROCK_API_KEY: ${AWS_BEDROCK_API_KEY:-}
      AWS_BEDROCK_REGION: ${AWS_BEDROCK_REGION:-us-east-1}
      AWS_BEDROCK_MODEL_ID: ${AWS_BEDROCK_MODEL_ID:-anthropic.claude-3-5-sonnet-20241022-v2:0}

      # ============================================================
      # LLAMA.CPP CONFIGURATION (OPTIONAL)
      # ============================================================
      # For local GGUF models
      LLAMACPP_ENDPOINT: ${LLAMACPP_ENDPOINT:-http://localhost:8080}
      LLAMACPP_MODEL: ${LLAMACPP_MODEL:-default}
      LLAMACPP_EMBEDDINGS_ENDPOINT: ${LLAMACPP_EMBEDDINGS_ENDPOINT:-http://localhost:8080/embeddings}
      LLAMACPP_TIMEOUT_MS: ${LLAMACPP_TIMEOUT_MS:-120000}

      # ============================================================
      # OPENAI CONFIGURATION (OPTIONAL)
      # ============================================================
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      OPENAI_MODEL: ${OPENAI_MODEL:-gpt-4o}
      OPENAI_ENDPOINT: ${OPENAI_ENDPOINT:-https://api.openai.com/v1/chat/completions}

      # ============================================================
      # EMBEDDINGS PROVIDER OVERRIDE (OPTIONAL)
      # ============================================================
      # Options: ollama, llamacpp, openrouter, openai
      # By default, uses same provider as MODEL_PROVIDER
      EMBEDDINGS_PROVIDER: ${EMBEDDINGS_PROVIDER:-}

      # ============================================================
      # SERVER CONFIGURATION
      # ============================================================
      PORT: ${PORT:-8081}
      LOG_LEVEL: ${LOG_LEVEL:-info}
      WEB_SEARCH_ENDPOINT: ${WEB_SEARCH_ENDPOINT:-http://searxng:8888/search}
      WORKSPACE_ROOT: /workspace

      # ============================================================
      # PRODUCTION HARDENING (OPTIONAL)
      # ============================================================
      CIRCUIT_BREAKER_FAILURE_THRESHOLD: ${CIRCUIT_BREAKER_FAILURE_THRESHOLD:-5}
      CIRCUIT_BREAKER_TIMEOUT: ${CIRCUIT_BREAKER_TIMEOUT:-60000}
      LOAD_SHEDDING_MEMORY_THRESHOLD: ${LOAD_SHEDDING_MEMORY_THRESHOLD:-0.85}

      # ============================================================
      # HEADROOM CONTEXT COMPRESSION (OPTIONAL)
      # ============================================================
      # Provides 47-92% token reduction through intelligent compression
      HEADROOM_ENABLED: ${HEADROOM_ENABLED:-false}
      HEADROOM_ENDPOINT: ${HEADROOM_ENDPOINT:-http://headroom:8787}
      HEADROOM_TIMEOUT_MS: ${HEADROOM_TIMEOUT_MS:-5000}
      HEADROOM_MIN_TOKENS: ${HEADROOM_MIN_TOKENS:-500}
      HEADROOM_MODE: ${HEADROOM_MODE:-optimize}
      # Disable Docker management - we use docker-compose instead
      HEADROOM_DOCKER_ENABLED: "false"
      # Transform settings
      HEADROOM_SMART_CRUSHER: ${HEADROOM_SMART_CRUSHER:-true}
      HEADROOM_TOOL_CRUSHER: ${HEADROOM_TOOL_CRUSHER:-true}
      HEADROOM_CACHE_ALIGNER: ${HEADROOM_CACHE_ALIGNER:-true}
      HEADROOM_ROLLING_WINDOW: ${HEADROOM_ROLLING_WINDOW:-true}
      HEADROOM_KEEP_TURNS: ${HEADROOM_KEEP_TURNS:-3}
      HEADROOM_CCR: ${HEADROOM_CCR:-true}
      HEADROOM_CCR_TTL: ${HEADROOM_CCR_TTL:-300}
      HEADROOM_LLMLINGUA: ${HEADROOM_LLMLINGUA:-false}

    volumes:
      - ./data:/app/data  # Persist SQLite databases
      - .:/workspace      # Mount workspace
    restart: unless-stopped
    networks:
      - lynkr-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    labels:
      - "com.lynkr.version=1.0.1"
      - "com.lynkr.description=Claude Code proxy with multi-provider support"
    # Uncomment to set resource limits
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '2'
    #       memory: 2G
    #     reservations:
    #       cpus: '0.5'
    #       memory: 512M

  # Ollama service
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    profiles:
      - ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama  # Persist downloaded models
    restart: unless-stopped
    networks:
      - lynkr-network
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      - "com.lynkr.service=ollama"
      - "com.lynkr.description=Local LLM runtime"
    # Uncomment for NVIDIA GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # Optional: Ollama Web UI (if you want a visual interface)
  # ollama-webui:
  #   image: ghcr.io/open-webui/open-webui:main
  #   container_name: ollama-webui
  #   ports:
  #     - "3000:8080"
  #   environment:
  #     OLLAMA_BASE_URL: http://ollama:11434
  #   volumes:
  #     - ollama-webui-data:/app/backend/data
  #   depends_on:
  #     ollama:
  #       condition: service_healthy
  #   restart: unless-stopped
  #   networks:
  #     - lynkr-network
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

  # Local searxng search service (web search provider)
  searxng:
    image: searxng/searxng:latest
    container_name: searxng
    ports:
      - "8888:8080"
    volumes:
      - searxng-data:/etc/searxng
#    environment:
      # you can add SEARX settings here if needed (e.g. timezone, settings.yml mount, etc.)
      # see https://docs.searxng.org/admin/installation-docker.html#environment-variables
    dns:
      - 8.8.8.8
      - 1.1.1.1
    restart: unless-stopped
    networks:
      - lynkr-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # Headroom context compression sidecar (47-92% token reduction)
  headroom:
    image: lynkr/headroom-sidecar:latest
    container_name: lynkr-headroom
    profiles:
      - headroom
    build:
      context: ./headroom-sidecar
      dockerfile: Dockerfile
    ports:
      - "8787:8787"
    environment:
      HEADROOM_HOST: "0.0.0.0"
      HEADROOM_PORT: "8787"
      HEADROOM_LOG_LEVEL: ${HEADROOM_LOG_LEVEL:-info}
      HEADROOM_MODE: ${HEADROOM_MODE:-optimize}
      HEADROOM_PROVIDER: ${HEADROOM_PROVIDER:-anthropic}
      # Transforms
      HEADROOM_SMART_CRUSHER: ${HEADROOM_SMART_CRUSHER:-true}
      HEADROOM_SMART_CRUSHER_MIN_TOKENS: ${HEADROOM_SMART_CRUSHER_MIN_TOKENS:-200}
      HEADROOM_SMART_CRUSHER_MAX_ITEMS: ${HEADROOM_SMART_CRUSHER_MAX_ITEMS:-15}
      HEADROOM_TOOL_CRUSHER: ${HEADROOM_TOOL_CRUSHER:-true}
      HEADROOM_CACHE_ALIGNER: ${HEADROOM_CACHE_ALIGNER:-true}
      HEADROOM_ROLLING_WINDOW: ${HEADROOM_ROLLING_WINDOW:-true}
      HEADROOM_KEEP_TURNS: ${HEADROOM_KEEP_TURNS:-3}
      # CCR
      HEADROOM_CCR: ${HEADROOM_CCR:-true}
      HEADROOM_CCR_TTL: ${HEADROOM_CCR_TTL:-300}
      # LLMLingua (optional, requires GPU)
      HEADROOM_LLMLINGUA: ${HEADROOM_LLMLINGUA:-false}
      HEADROOM_LLMLINGUA_DEVICE: ${HEADROOM_LLMLINGUA_DEVICE:-auto}
    volumes:
      - headroom-data:/app/data
    restart: unless-stopped
    networks:
      - lynkr-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8787/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      - "com.lynkr.service=headroom"
      - "com.lynkr.description=Context compression sidecar"
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    # Uncomment for GPU support (LLMLingua)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

volumes:
  ollama-data:
    driver: local
  # ollama-webui-data:
  #   driver: local
  searxng-data:
    driver: local
  headroom-data:
    driver: local

networks:
  lynkr-network:
    driver: bridge
