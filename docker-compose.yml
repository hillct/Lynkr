version: "3.9"

services:
  # Lynkr proxy service
  lynkr:
    build: .
    container_name: lynkr
    ports:
      - "8080:8080"
      - "8888:8888"
    environment:
      # ============================================================
      # PRIMARY MODEL PROVIDER
      # ============================================================
      # Options: ollama, databricks, azure-openai, azure-anthropic, openrouter
      # - ollama: Local models (free, private, offline)
      # - databricks: Claude Sonnet 4.5, Opus 4.5 (production)
      # - azure-openai: GPT-4o, GPT-5, o1, o3 (Azure integration)
      # - azure-anthropic: Claude models via Azure
      # - openrouter: 100+ models (flexible, cost-effective)
      MODEL_PROVIDER: ${MODEL_PROVIDER:-ollama}

      # ============================================================
      # TOOL EXECUTION MODE
      # ============================================================
      # Options: server (default), client (passthrough mode)
      # - server: Tools execute on proxy server
      # - client: Tools execute on Claude Code CLI (client-side)
      TOOL_EXECUTION_MODE: ${TOOL_EXECUTION_MODE:-server}

      # ============================================================
      # OLLAMA CONFIGURATION (Local Models)
      # ============================================================
      # Recommended models for tool calling:
      # - llama3.1:8b (good balance)
      # - llama3.2 (latest)
      # - qwen2.5:14b (strong reasoning, 7b struggles with tools)
      # - mistral:7b-instruct (fast and capable)
      PREFER_OLLAMA: ${PREFER_OLLAMA:-true}
      OLLAMA_ENDPOINT: http://ollama:11434
      OLLAMA_MODEL: ${OLLAMA_MODEL:-llama3.1:8b}
      OLLAMA_MAX_TOOLS_FOR_ROUTING: ${OLLAMA_MAX_TOOLS_FOR_ROUTING:-3}

      # ============================================================
      # OPENROUTER CONFIGURATION
      # ============================================================
      OPENROUTER_API_KEY: ${OPENROUTER_API_KEY:-}
      OPENROUTER_MODEL: ${OPENROUTER_MODEL:-amazon/nova-2-lite-v1:free}
      OPENROUTER_ENDPOINT: ${OPENROUTER_ENDPOINT:-https://openrouter.ai/api/v1/chat/completions}
      OPENROUTER_MAX_TOOLS_FOR_ROUTING: ${OPENROUTER_MAX_TOOLS_FOR_ROUTING:-15}

      # ============================================================
      # AZURE OPENAI CONFIGURATION
      # ============================================================
      # Required when MODEL_PROVIDER=azure-openai
      # Deployment options: gpt-4o, gpt-4o-mini, gpt-5-chat, o1-preview, o3-mini
      # Get credentials from: https://portal.azure.com → Azure OpenAI → Keys and Endpoint
      AZURE_OPENAI_ENDPOINT: ${AZURE_OPENAI_ENDPOINT:-}
      AZURE_OPENAI_API_KEY: ${AZURE_OPENAI_API_KEY:-}
      AZURE_OPENAI_DEPLOYMENT: ${AZURE_OPENAI_DEPLOYMENT:-gpt-4o}
      AZURE_OPENAI_API_VERSION: ${AZURE_OPENAI_API_VERSION:-2024-08-01-preview}

      # ============================================================
      # HYBRID ROUTING & FALLBACK
      # ============================================================
      # Enable/disable fallback to cloud providers
      FALLBACK_ENABLED: ${FALLBACK_ENABLED:-true}
      # Fallback provider when Ollama can't handle request
      # Options: databricks, azure-openai, azure-anthropic, openrouter
      FALLBACK_PROVIDER: ${FALLBACK_PROVIDER:-databricks}

      # ============================================================
      # DATABRICKS CONFIGURATION
      # ============================================================
      DATABRICKS_API_BASE: ${DATABRICKS_API_BASE:-https://example.cloud.databricks.com}
      DATABRICKS_API_KEY: ${DATABRICKS_API_KEY:-replace-with-databricks-pat}

      # ============================================================
      # AZURE ANTHROPIC CONFIGURATION (OPTIONAL)
      # ============================================================
      AZURE_ANTHROPIC_ENDPOINT: ${AZURE_ANTHROPIC_ENDPOINT:-}
      AZURE_ANTHROPIC_API_KEY: ${AZURE_ANTHROPIC_API_KEY:-}

      # ============================================================
      # SERVER CONFIGURATION
      # ============================================================
      PORT: ${PORT:-8080}
      LOG_LEVEL: ${LOG_LEVEL:-info}
      WEB_SEARCH_ENDPOINT: ${WEB_SEARCH_ENDPOINT:-http://localhost:8888/search}
      WORKSPACE_ROOT: /workspace
    volumes:
      - ./data:/app/data  # Persist SQLite databases
      - .:/workspace      # Mount workspace
    depends_on:
      - ollama
    restart: unless-stopped
    networks:
      - lynkr-network

  # Ollama service
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama  # Persist downloaded models
    restart: unless-stopped
    networks:
      - lynkr-network
    # Uncomment for NVIDIA GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # Optional: Ollama Web UI (if you want a visual interface)
  # ollama-webui:
  #   image: ghcr.io/open-webui/open-webui:main
  #   container_name: ollama-webui
  #   ports:
  #     - "3000:8080"
  #   environment:
  #     OLLAMA_BASE_URL: http://ollama:11434
  #   volumes:
  #     - ollama-webui-data:/app/backend/data
  #   depends_on:
  #     - ollama
  #   restart: unless-stopped
  #   networks:
  #     - lynkr-network

volumes:
  ollama-data:
    driver: local
  # ollama-webui-data:
  #   driver: local

networks:
  lynkr-network:
    driver: bridge
